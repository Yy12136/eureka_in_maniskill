["import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Define reward weights\n    weight_grasp = 0.3\n    weight_lift = 0.4\n    weight_stability = 0.2\n    weight_effort = 0.1\n    \n    # Initialize components\n    reward_grasp = 0.0  # Reward for successfully grasping the cube\n    reward_lift = 0.0   # Reward for lifting the cube to the target height\n    reward_stability = 0.0  # Reward for maintaining stability during the task\n    reward_effort = 0.0  # Reward for minimizing joint effort\n    \n    # Calculate reward_grasp: Successful grasp\n    if self.agent.check_grasp(self.obj):\n        reward_grasp = 1.0\n    \n    # Calculate reward_lift: Lift the cube by 0.2 meters\n    cube_height = self.obj.pose.p[2]  # Z-coordinate of the cube\n    target_height = 0.2\n    height_diff = max(0, target_height - cube_height)\n    reward_lift = 1.0 - (height_diff / target_height)\n    \n    # Calculate reward_stability: Cube should remain static during lifting\n    if check_actor_static(self.obj):\n        reward_stability = 1.0\n    \n    # Calculate reward_effort: Minimize joint effort (velocity and position)\n    joint_effort = sum(abs(self.agent.robot.get_qvel()[:-2])) + sum(abs(self.agent.robot.get_qpos()[:-2]))\n    reward_effort = 1.0 / (1.0 + joint_effort)\n    \n    # Combine all rewards\n    reward = (\n        weight_grasp * reward_grasp +\n        weight_lift * reward_lift +\n        weight_stability * reward_stability +\n        weight_effort * reward_effort\n    )\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Define reward weights\n    weight_grasp = 0.3\n    weight_lift = 0.4\n    weight_stability = 0.2\n    weight_effort = 0.1\n    \n    # Initialize components\n    reward_grasp = 0.0  # Reward for successfully grasping cube A\n    reward_lift = 0.0  # Reward for lifting cube A by 0.2 meters\n    reward_stability = 0.0  # Reward for maintaining stability during the task\n    reward_effort = 0.0  # Reward for minimizing joint effort\n    \n    # Calculate each component\n    \n    # Grasp reward: Check if cube A is grasped\n    if self.agent.check_grasp(self.obj):\n        reward_grasp = 1.0\n    \n    # Lift reward: Check if cube A is lifted by 0.2 meters\n    cube_height = self.obj.pose.p[2]  # Z-coordinate of cube A\n    initial_height = 0.02  # Initial height of cube A (assuming it starts on the ground)\n    target_height = initial_height + 0.2  # Target height is 0.2 meters above initial height\n    if cube_height >= target_height:\n        reward_lift = 1.0\n    else:\n        reward_lift = max(0, (cube_height - initial_height) / (target_height - initial_height))\n    \n    # Stability reward: Check if cube A is static (not moving)\n    if check_actor_static(self.obj):\n        reward_stability = 1.0\n    \n    # Effort reward: Minimize joint effort (sum of squared joint velocities)\n    joint_effort = sum(qv**2 for qv in self.agent.robot.get_qvel()[:-2])\n    reward_effort = max(0, 1 - joint_effort)  # Normalize between 0 and 1\n    \n    # Combine all rewards\n    reward = (\n        weight_grasp * reward_grasp +\n        weight_lift * reward_lift +\n        weight_stability * reward_stability +\n        weight_effort * reward_effort\n    )\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Define reward weights\n    weight_approach = 0.3\n    weight_grasp = 0.3\n    weight_lift = 0.4\n    \n    # Initialize components\n    reward_approach = 0.0  # Reward for approaching the cube\n    reward_grasp = 0.0     # Reward for successfully grasping the cube\n    reward_lift = 0.0      # Reward for lifting the cube by 0.2 meters\n    \n    # Calculate approach reward\n    # Encourage the robot to move closer to the cube\n    cube_pos = self.obj.pose.p\n    ee_pos = self.tcp.pose.p\n    distance = np.linalg.norm(cube_pos - ee_pos)\n    reward_approach = max(0, 1 - distance / (0.02 * 2))\n    \n    # Calculate grasp reward\n    # Reward the robot for successfully grasping the cube\n    if self.agent.check_grasp(self.obj):\n        reward_grasp = 1.0\n    \n    # Calculate lift reward\n    # Reward the robot for lifting the cube by 0.2 meters\n    if self.agent.check_grasp(self.obj):\n        lifted_height = self.obj.pose.p[2] - 0.02\n        reward_lift = max(0, min(1, lifted_height / 0.2))\n    \n    # Combine all rewards\n    reward = (\n        weight_approach * reward_approach +\n        weight_grasp * reward_grasp +\n        weight_lift * reward_lift\n    )\n    \n    # Regularization to encourage smooth and stable movements\n    # Penalize high velocities and large joint movements\n    qvel_norm = np.linalg.norm(self.agent.robot.get_qvel()[:-2])\n    action_norm = np.linalg.norm(action)\n    regularization = -0.01 * (qvel_norm + action_norm)\n    \n    reward += regularization\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Define reward weights\n    weight_approach = 0.3\n    weight_grasp = 0.3\n    weight_lift = 0.4\n    \n    # Initialize components\n    reward_approach = 0.0  # Reward for approaching cube A\n    reward_grasp = 0.0     # Reward for successfully grasping cube A\n    reward_lift = 0.0      # Reward for lifting cube A by 0.2 meters\n    \n    # Calculate reward_approach: Encourage the robot to move closer to cube A\n    cubeA_pos = self.obj.pose.p\n    ee_pos = self.tcp.pose.p\n    distance_to_cubeA = np.linalg.norm(ee_pos - cubeA_pos)\n    reward_approach = max(0, 1 - distance_to_cubeA / (2 * 0.02))\n    \n    # Calculate reward_grasp: Reward successful grasping of cube A\n    if self.agent.check_grasp(self.obj):\n        reward_grasp = 1.0\n    \n    # Calculate reward_lift: Reward lifting cube A by 0.2 meters\n    if self.agent.check_grasp(self.obj):\n        cubeA_height = self.obj.pose.p[2]\n        target_height = 0.2\n        height_diff = max(0, cubeA_height - target_height)\n        reward_lift = max(0, 1 - height_diff / target_height)\n    \n    # Combine all rewards\n    reward = (\n        weight_approach * reward_approach +\n        weight_grasp * reward_grasp +\n        weight_lift * reward_lift\n    )\n    \n    # Regularization: Penalize large joint velocities to encourage smooth motion\n    joint_velocity_penalty = -0.01 * np.linalg.norm(self.agent.robot.get_qvel()[:-2])\n    reward += joint_velocity_penalty\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Define reward weights\n    weight_approach = 0.3\n    weight_grasp = 0.3\n    weight_lift = 0.4\n    \n    # Initialize components\n    reward_approach = 0.0  # Reward for approaching the cube\n    reward_grasp = 0.0     # Reward for successfully grasping the cube\n    reward_lift = 0.0      # Reward for lifting the cube by 0.2 meters\n    \n    # Calculate reward_approach: Encourage the robot to move closer to the cube\n    ee_pos = self.tcp.pose[:3]\n    cube_pos = self.obj.pose[:3]\n    distance_to_cube = np.linalg.norm(ee_pos - cube_pos)\n    reward_approach = max(0, 1.0 - distance_to_cube / 0.5)  # Normalized to [0, 1]\n    \n    # Calculate reward_grasp: Reward for successful grasp\n    if self.agent.check_grasp(self.obj):\n        reward_grasp = 1.0\n    \n    # Calculate reward_lift: Reward for lifting the cube by 0.2 meters\n    if self.agent.check_grasp(self.obj):\n        cube_height = self.obj.pose[2]\n        initial_height = cube_pos[2]\n        height_difference = cube_height - initial_height\n        reward_lift = max(0, min(1.0, height_difference / 0.2))  # Normalized to [0, 1]\n    \n    # Combine all rewards\n    reward = weight_approach * reward_approach + weight_grasp * reward_grasp + weight_lift * reward_lift\n    \n    return reward"]